for (i in 1:20){
date20[o+i] = date[o]+i
}
h = D_expSM(x,0.1,0.1)
plot(h ~ date20, type="l",ylab="x",main=c("alpha=0.1", "beta=0.1"), col = "blue")
legend("topleft", legend =c("lina","Actual data","Predicted"),col = c("black","red","blue"), lty = 1)
lines(x20 ~ date20, col="red")
abline(v = date[p])
?abline
plot(h ~ date20, type="l",ylab="x",main=c("alpha=0.1", "beta=0.1"), col = "blue")
legend("topleft", legend =c("lina","Actual data","Predicted"),col = c("black","red","blue"), lty = 1)
lines(x20 ~ date20, col="red")
abline(v = date[p], \dots)
abline(v = date[p])
plot(h ~ date20, type="l",ylab="x",main=c("alpha=0.1", "beta=0.1"), col = "blue")
legend("topleft", legend =c("lina","Actual data","Predicted"),col = c("black","red","blue"), lty = 1)
lines(x20 ~ date20, col="red")
abline(v = date[p],untf = FALSE, \dots)
plot(h ~ date20, type="l",ylab="x",main=c("alpha=0.1", "beta=0.1"), col = "blue")
legend("topleft", legend =c("lina","Actual data","Predicted"),col = c("black","red","blue"), lty = 1)
lines(x20 ~ date20, col="red")
abline(v = date[p], lty="dashed")
plot(h ~ date20, type="l",ylab="x",main=c("alpha=0.1", "beta=0.1"), col = "blue")
legend("topleft", legend =c("lina","Actual data","Predicted","alpha=0.1"),col = c("black","red","blue"), lty = 1)
lines(x20 ~ date20, col="red")
abline(v = date[p], lty="dashed")
legend("topleft", legend =c("End of training data","Actual data","Predicted"),col = c("black","red","blue"), lty = 1)
lines(x20 ~ date20, col="red")
abline(v = date[p], lty="dashed")
plot(h ~ date20, type="l",ylab="x",main=c("Actual data vs. predicted","alpha=0.1, beta=0.1"), col = "blue")
legend("topleft", legend =c("End of training data","Actual data","Predicted"),col = c("black","red","blue"), lty = 1)
lines(x20 ~ date20, col="red")
abline(v = date[p], lty="dashed")
?plot
plot(h ~ date20, type="l",ylab="x",main=c("Actual data vs. predicted","alpha=0.1, beta=0.1"), col = "blue",lwd = 10)
lines(x20 ~ date20, col="red")
abline(v = date[p], lty="dashed")
legend("topleft", legend =c("End of training data","Actual data","Predicted"),col = c("black","red","blue"), lty = 1)
plot(h ~ date20, type="l",ylab="x",main=c("Actual data vs. predicted","alpha=0.1, beta=0.1"), col = "blue",lwd = 2)
lines(x20 ~ date20, col="red")
abline(v = date[p], lty="dashed")
legend("topleft", legend =c("End of training data","Actual data","Predicted"),col = c("black","red","blue"), lty = 1)
URL <- "http://datamarket.com/api/v1/list.csv?ds=yfz!6zu=4&dates_as_dates=1&sharing_key=3cff9bd7e76443b5b416ce1e757fb557"
gamma <- read.csv(URL)
gamma$Date <- as.Date(gamma$Date, "%Y-%m-%d")
# Question 1
#       Plot the time series.
#
plot(gamma$Value ~gamma$Date,type="l", main="Gamma stock price index")
grid(nx=12, ny=5)
# Question 2
#       Split the time series into a training set and a test set.
#
training_set <- subset(gamma, (gamma$Date > '2011-01-01') & (gamma$Date <= '2013-12-31'))
test_set <- subset(gamma,gamma$Date > '2013-12-31')
# Question 3
#       Choose a model and make a prediction 20 steps ahead.
#
D_expSM <- function(data,alpha,beta){   #Double exponential smoothing
s = NULL
b = NULL
f = NULL
s[2] = data[2]
b[2] = data[2]-data[1]
b[1] = b[2]
for(i in 3:length(data)){
s[i] = alpha*data[i] + (1-alpha)*(s[i-1] + b[i-1])
b[i] = beta*(s[i]-s[i-1]) + (1-beta)*b[i-1]
f[i+1] = s[i]+b[i]
}
for(j in 1:20){                                 # Ans Q4
f[length(data)+j+1] = s[length(data)+1]+b[length(data)+1]  # F_t+m = s_t + m*b_t
}                                               # reikna 20 skref fram Ãƒ­ timann
return(f)
}
x = rev(training_set$Value)
test_x = rev(test_set$Value)
x20 = c(x,test_x[1:20])
date = rev(training_set$Date)
date20 = date
p = length(date)
o = length(date20)
for (i in 1:20){
date20[o+i] = date[o]+i
}
h = D_expSM(x,0.1,0.1)
plot(h ~ date20, type="l",ylab="x",main=c("Actual data vs. predicted","alpha=0.1, beta=0.1"), col = "blue",lwd = 2)
lines(x20 ~ date20, col="red")
abline(v = date[p], lty="dashed")
legend("topleft", legend =c("End of training data","Actual data","Predicted"),col = c("black","red","blue"), lty = 1)
# Comments: Our initial thougth was to use Holt Winters (Triple exp smoothing) but since there is no obvious seasonal
# factor in our data (but definetally a trend) we decided to use Double exp smoothing to make the forecast.
#
plot(gamma$Value ~gamma$Date,type="l", main="Gamma stock price index")
grid(nx=12, ny=5)
plot(h ~ date20, type="l",ylab="x",main=c("Actual data vs. predicted","alpha=0.1, beta=0.1"), col = "blue",lwd = 2)
x = rev(training_set$Value)
test_x = rev(test_set$Value)
x20 = c(x,test_x[1:20])
date = rev(training_set$Date)
date20 = date
p = length(date)
o = length(date20)
for (i in 1:20){
date20[o+i] = date[o]+i
}
h = D_expSM(x,0.1,0.1)
plot(h ~ date20, type="l",ylab="x",main=c("Actual data vs. predicted","alpha=0.1, beta=0.1"), col = "blue",lwd = 2)
f
h
x = rev(training_set$Value)
test_x = rev(test_set$Value)
x20 = c(x,test_x[1:20])
date = rev(training_set$Date)
date20 = date
p = length(date)
o = length(date20)
for (i in 1:20){
date20[o+i] = date[o]+i
}
h = D_expSM(x,0.1,0.1)
plot(h ~ date20, type="l",ylab="x",main=c("Actual data vs. predicted","alpha=0.1, beta=0.1"), col = "blue",lwd = 2)
lines(x20 ~ date20, col="red")
abline(v = date[p], lty="dashed")
legend("topleft", legend =c("End of training data","Actual data","Predicted"),col = c("black","red","blue"), lty = 1)
# Comments: Our initial thougth was to use Holt Winters (Triple exp smoothing) but since there is no obvious seasonal
URL <- "http://datamarket.com/api/v1/list.csv?ds=yfz!6zu=4&dates_as_dates=1&sharing_key=3cff9bd7e76443b5b416ce1e757fb557"
gamma <- read.csv(URL)
gamma$Date <- as.Date(gamma$Date, "%Y-%m-%d")
# Question 1
#       Plot the time series.
#
plot(gamma$Value ~gamma$Date,type="l", main="Gamma stock price index")
grid(nx=12, ny=5)
# Question 2
#       Split the time series into a training set and a test set.
#
training_set <- subset(gamma, (gamma$Date > '2011-01-01') & (gamma$Date <= '2013-12-31'))
test_set <- subset(gamma,gamma$Date > '2013-12-31')
# Question 3
#       Choose a model and make a prediction 20 steps ahead.
#
D_expSM <- function(data,alpha,beta){   #Double exponential smoothing
s = NULL
b = NULL
f = NULL
s[2] = data[2]
b[2] = data[2]-data[1]
b[1] = b[2]
for(i in 3:length(data)){
s[i] = alpha*data[i] + (1-alpha)*(s[i-1] + b[i-1])
b[i] = beta*(s[i]-s[i-1]) + (1-beta)*b[i-1]
}
for(j in 1:20){                                 # Ans Q4
s[length(data)+j] = s[length(data)]+j*b[length(data)]  # F_t+m = s_t + m*b_t
}                                               # reikna 20 skref fram Ãƒ­ timann
return(s)
}
x = rev(training_set$Value)
test_x = rev(test_set$Value)
x20 = c(x,test_x[1:20])
date = rev(training_set$Date)
date20 = date
p = length(date)
o = length(date20)
for (i in 1:20){
date20[o+i] = date[o]+i
}
h = D_expSM(x,0.1,0.1)
plot(h ~ date20, type="l",ylab="x",main=c("Actual data vs. predicted","alpha=0.1, beta=0.1"), col = "blue",lwd = 2)
lines(x20 ~ date20, col="red")
abline(v = date[p], lty="dashed")
legend("topleft", legend =c("End of training data","Actual data","Predicted"),col = c("black","red","blue"), lty = 1)
# Comments: Our initial thougth was to use Holt Winters (Triple exp smoothing) but since there is no obvious seasonal
# factor in our data (but definetally a trend) we decided to use Double exp smoothing to make the forecast.
legend("topleft", legend =c("End of training data","Actual data","Smoothed + Predicted"),col = c("black","red","blue"), lty = 1)
setwd("~/Documents/GitHub/HRTimeSeries-project2")
require("forecast")
data = read.csv("veks.csv")
data$HC.f = ts(data$HC.f,frequency = 24, start = c(1995,((data$ds.diy[1]*24)+data$ds.hh[1])))
data$Ta.f = ts(data$Ta.f, frequency = 24, start =c( 1995,((data$ds.diy[1]*24)+data$ds.hh[1])))
data$W.f = ts(data$W.f, frequency = 24, start = c(1995,((data$ds.diy[1]*24)+data$ds.hh[1])))
data$GR.f = ts(data$GR.f, frequency =24, start = c(1995,((data$ds.diy[1]*24)+data$ds.hh[1])))
time = as.POSIXct("1960-1-1") + (data$jdate*24 + data$hh) *3600
#Consider the time series of heat consumption.
par(mfrow=c(2,1))
plot.ts(data$HC.f ,
type='l',
main='Original Heat consumption(GJ/h) data ',
xlab="running time",
ylab="Heat consumption [GJ/h]",
col="red")
grid()
plot.ts(diff(data$HC.f, difference=1) ,
type='l',
main='Differenced Heat consumption(GJ/h) data ',
xlab="running time",
ylab="Heat consumption [GJ/h]",
col="red")
grid()
WD = NULL
for(i in 1:length(data$ds.dow)){
if(data$ds.tod[i] == 1){
WD[i] = 0
}
else{
WD[i] = 1
}
}
data$ext_regressors <- cbind(WD=data$WD,Ta.f=data$Ta.f,GR.f=data$GR.f,W.f=data$W.f)
fit3 = Arima(training_set$HC.f,
order=c(2,1,1),
seasonal = list(order = c(1,0,1), period = 24),
xreg = training_set$ext_regressors[,1] #working days regressor
)
training_set = subset(data, data$row.names <= 6000)
test_set = subset(data,data$row.names > 6000)
time_training = time[1:6000]
time_test = time[6001:length(time)]
fit3 = Arima(training_set$HC.f,
order=c(2,1,1),
seasonal = list(order = c(1,0,1), period = 24),
xreg = training_set$ext_regressors[,1] #working days regressor
)
forecast_1ahead = forecast.Arima(fit3,fan=TRUE, h=1, xreg= test_set$ext_regressors[1,1])
plot.forecast(forecast_1ahead,
include=24,
type='l',
fcol="red",
col='blue',
main="Forecast, 1 hour ahead")
lines(c(training_set$HC.f, test_set$HC.f[1]))
forecast_6ahead= forecast.Arima(fit3,fan=TRUE, h=6, xreg= test_set$ext_regressors[1:6,1])
plot.forecast(forecast_6ahead,
include=24,
type='l',
fcol="red",
col='blue',
main="Forecast, 6 hour ahead")
lines(c(training_set$HC.f, test_set$HC.f[1:6]))
dev.off()
plot.ts(data$Ta.f,
main="Ambient air temperature data",
type='l',
ylab='Centigrade',
col='red')
grid()
plot.ts(diff(data$Ta.f,difference=1),
main="Ambient air temperature data (first difference)",
type='l',
ylab='Centigrade',
col='red')
setwd("~/Documents/GitHub/HRTimeSeries-project2")
require("forecast")
data = read.csv("veks.csv")
data$HC.f = ts(data$HC.f,frequency = 24, start = c(1995,((data$ds.diy[1]*24)+data$ds.hh[1])))
data$Ta.f = ts(data$Ta.f, frequency = 24, start =c( 1995,((data$ds.diy[1]*24)+data$ds.hh[1])))
data$W.f = ts(data$W.f, frequency = 24, start = c(1995,((data$ds.diy[1]*24)+data$ds.hh[1])))
data$GR.f = ts(data$GR.f, frequency =24, start = c(1995,((data$ds.diy[1]*24)+data$ds.hh[1])))
time = as.POSIXct("1960-1-1") + (data$jdate*24 + data$hh) *3600
par(mfrow=c(2,1))
plot.ts(data$HC.f ,
type='l',
main='Original Heat consumption(GJ/h) data ',
xlab="running time",
ylab="Heat consumption [GJ/h]",
col="red")
grid()
plot.ts(diff(data$HC.f, difference=1) ,
type='l',
main='Differenced Heat consumption(GJ/h) data ',
xlab="running time",
ylab="Heat consumption [GJ/h]",
col="red")
grid()
WD = NULL
for(i in 1:length(data$ds.dow)){
if(data$ds.tod[i] == 1){
WD[i] = 0
}
else{
WD[i] = 1
}
}
data$ext_regressors <- cbind(WD=data$WD,Ta.f=data$Ta.f,GR.f=data$GR.f,W.f=data$W.f)
training_set = subset(data, data$row.names <= 6000)
test_set = subset(data,data$row.names > 6000)
time_training = time[1:6000]
time_test = time[6001:length(time)]
#Estimate the autocovariance, autocorrelation and partial autocorrelation
#functions for the heat consumption.
dev.off()
par(mfrow = c(2,2))
acf(diff(data$HC.f, difference=1))
pacf(diff(data$HC.f, difference=1))
acf(diff(data$HC.f, difference=1), type="covariance")
#Estimate the spectrum for the variations of the heat consumption.
spectrum(diff(data$HC.f, difference=1),
method = "ar")
#You should comment on the estimated correlation functions
#and spectrum, e.g. what can you tell about the process itself,
#about a good choice of model structure for modelling the process, etc..
#Comment: From the ACF and PACF, we assume we can model the seris as ARIMA(8,2) from table 6.1
#     This is probably too many parameters. Thus we suspect there is seaonality, we need to look at seasonal variations
#     The spectrum reinforces this theory that there are still some variations in higher frequencies.
par(mfrow = c(2,1))
acf(diff(data$HC.f, difference=1), lag.max=300)
pacf(diff(data$HC.f, difference=1), lag.max=300)
#Comment: We can see 24hour periodic correlations in the data, and some weekly variations
# We want our model to include 24hour and weekly seasonality
# We will accomodate weekend seasonality with a vactor of external regressors.
# We have made this regressor earlier in data$ext_regressors[,1]
dev.off()
fit1 = Arima(training_set$HC.f,
order=c(1,1,1),
seasonal = list(order = c(1,0,1),period = 24),
xreg = training_set$ext_regressors[,1] #working days regressors
)
fit1
acf(fit1$residuals,lag.max=50)
#We discard this model, because the residuals are not within the confidence interval
fit2 = Arima(training_set$HC.f,
order=c(1,1,2),
seasonal = list(order = c(1,0,1), period = 24),
xreg = training_set$ext_regressors[,1]  # working days regressor
)
acf(fit2$residuals, lag.max=50)
fit2
#We discard this model, because the residuals are not within the confidence interval
fit3 = Arima(training_set$HC.f,
order=c(2,1,1),
seasonal = list(order = c(1,0,1), period = 24),
xreg = training_set$ext_regressors[,1] #working days regressor
)
acf(fit3$residuals)
fit3
#We like this this model, because the residuals are more or less within the confidence interval and AIC has the smallest value
# and are thus deemed to be white noise.
dev.off()
par(mfrow = c(2,1))
forecast_1ahead = forecast.Arima(fit3,fan=TRUE, h=1, xreg= test_set$ext_regressors[1,1])
plot.forecast(forecast_1ahead,
include=24,
type='l',
fcol="red",
col='blue',
main="Forecast, 1 hour ahead")
lines(c(training_set$HC.f, test_set$HC.f[1]))
forecast_6ahead= forecast.Arima(fit3,fan=TRUE, h=6, xreg= test_set$ext_regressors[1:6,1])
plot.forecast(forecast_6ahead,
include=24,
type='l',
fcol="red",
col='blue',
main="Forecast, 6 hour ahead")
lines(c(training_set$HC.f, test_set$HC.f[1:6]))
head(training_set$Ta.f)
head(training_set$Gr.f)
head(training_set$GR.f)
?cbind
head(data)
head(data)
?NA Action
?NA
?residual
?res
require("forecast")
data = read.csv("veks.csv")
data$HC.f = ts(data$HC.f,frequency = 24, start = c(1995,((data$ds.diy[1]*24)+data$ds.hh[1])))
data$Ta.f = ts(data$Ta.f, frequency = 24, start =c( 1995,((data$ds.diy[1]*24)+data$ds.hh[1])))
data$W.f = ts(data$W.f, frequency = 24, start = c(1995,((data$ds.diy[1]*24)+data$ds.hh[1])))
data$GR.f = ts(data$GR.f, frequency =24, start = c(1995,((data$ds.diy[1]*24)+data$ds.hh[1])))
time = as.POSIXct("1960-1-1") + (data$jdate*24 + data$hh) *3600
# ------------ Task 1 ----------------------
#Consider the time series of heat consumption.
par(mfrow=c(2,1))
plot.ts(data$HC.f ,
type='l',
main='Original Heat consumption(GJ/h) data ',
xlab="running time",
ylab="Heat consumption [GJ/h]",
col="red")
grid()
#Comment: we can see that the data set is nonstationary. Thus, we difference the series.
plot.ts(diff(data$HC.f, difference=1) ,
type='l',
main='Differenced Heat consumption(GJ/h) data ',
xlab="running time",
ylab="Heat consumption [GJ/h]",
col="red")
grid()
#Estimate the autocovariance, autocorrelation and partial autocorrelation
#functions for the heat consumption.
dev.off()
par(mfrow = c(2,2))
acf(diff(data$HC.f, difference=1))
pacf(diff(data$HC.f, difference=1))
acf(diff(data$HC.f, difference=1), type="covariance")
#Estimate the spectrum for the variations of the heat consumption.
spectrum(diff(data$HC.f, difference=1),
method = "ar")
WD = NULL
for(i in 1:length(data$ds.dow)){
if(data$ds.tod[i] == 1){
WD[i] = 0
}
else{
WD[i] = 1
}
}
data$ext_regressors =NULL
data$ext_regressors <- cbind(WD=WD)
training_set = subset(data, data$row.names <= 6000)
test_set = subset(data,data$row.names > 6000)
time_training = time[1:6000]
time_test = time[6001:length(time)]
par(mfrow = c(2,1))
acf(diff(data$HC.f, difference=1), lag.max=300)
pacf(diff(data$HC.f, difference=1), lag.max=300)
# Elaborate on your choice of model.  Your predictions should
dev.off()
fit1 = Arima(training_set$HC.f,
order=c(1,1,1),
seasonal = list(order = c(1,0,1),period = 24),
xreg = training_set$ext_regressors[,1] #working days regressors
)
fit1
fit3 = NULL
fit3 = Arima(training_set$HC.f,
order=c(2,1,1),
seasonal = list(order = c(1,0,1), period = 24),
xreg = training_set$ext_regressors[,1] #working days regressor
)
acf(fit3$residuals)
fit3
dev.off()
par(mfrow = c(2,1))
forecast_1ahead = forecast.Arima(fit3,fan=TRUE, h=1, xreg= test_set$ext_regressors[1,1])
plot.forecast(forecast_1ahead,
include=24,
type='l',
fcol="red",
col='blue',
main="Forecast, 1 hour ahead")
lines(c(training_set$HC.f, test_set$HC.f[1]))
forecast_6ahead= forecast.Arima(fit3,fan=TRUE, h=6, xreg= test_set$ext_regressors[1:6,1])
plot.forecast(forecast_6ahead,
include=24,
type='l',
fcol="red",
col='blue',
main="Forecast, 6 hour ahead")
lines(c(training_set$HC.f, test_set$HC.f[1:6]))
dev.off()
plot.ts(data$Ta.f,
main="Ambient air temperature data",
type='l',
ylab='Centigrade',
col='red')
grid()
plot.ts(diff(data$Ta.f,difference=1),
main="Ambient air temperature data (first difference)",
type='l',
ylab='Centigrade',
col='red')
grid()
dev.off()
ccf(diff(data$HC.f,difference=1),diff(data$Ta.f,difference=1),
main="cross correlation, HC vs amb temp",
col='red')
#Clearly, ambient temp shares common trend/seasonality with heatconsumtion data. Thus, we wish to prewhiten the amb temp series.
acf(diff(training_set$Ta.f))
temp.mdl <- arima(training_set$Ta.f,order=c(2,1,0),seasonal=list(order=c(1,0,0),period=24))
hct.res <- residuals(arima(training_set$HC.f,order=c(2,1,0),seasonal=list(order=c(1,0,0),period=24),fixed=temp.mdl$coef))
ccf(hct.res,temp.mdl$residuals)
grid()
Ta_lag1 = lag(training_set$Ta.f,1)
training_set$ext_regressors = cbind(WD=WD[1:6000],Ta_lag1=Ta_lag1)
dev.off()
plot(data$W.f ~time,
main="Wind speed data",
type='l',
ylab='m/s',
col='red')
grid()
plot(data$W.f,
main="Wind speed data",
type='l',
ylab='m/s',
col='red')
grid()
par(mfrow=c(2,1))
ccf(data$W.f,diff(data$HC.f,difference=1),  #cross corr, wind speed vs. HC
col='red',
main="cross correlation, HC vs wind speed")
ccf(data$W.f,diff(data$HC.f,difference=1),  #cross corr, wind speed vs. HC
lag.max=300,
col='red',
main="cross correlation, HC vs wind speed")
dev.off()
par(mfrow=c(2,1))
acf(diff(training_set$W.f))
pacf(diff(training_set$W.f))
#We make new model for external regressorb W.f
temp.mdl <- arima(training_set$W.f,order=c(2,0,0))
#Now we use this model to prewhiten our data (heat consumtion)
HC_Wind.res <- residuals(arima(training_set$HC.f,order=c(2,1,0),seasonal=list(order=c(1,0,0),period=24),fixed=c(temp.mdl$coef)))
HC_Wind.res = na.action(HC_Wind.res,na.rm=TRUE)
dev.off()
ccf(HC_Wind.res,temp.mdl$residuals)  #FUCKED LAG
data = read.csv("veks.csv")
data$HC.f = (data$HC.f,frequency = 24, start = c(1995,((data$ds.diy[1]*24)+data$ds.hh[1])))
